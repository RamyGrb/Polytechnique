{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Function Approximation\n",
    "\n",
    "Author: Ramy Ghorayeb\n",
    "    \n",
    "Date: December 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. On-Policy Reinforcement Learning with Parametric Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Trajectory-based formulation and REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_reward(rewards,gamma):\n",
    "    reward = 0\n",
    "    for r in rewards:\n",
    "        reward = (reward + r)*gamma\n",
    "    reward = reward/gamma\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Parametric policy models (Gaussian and Gibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian policy\n",
    "\n",
    "class Gaussian(object):\n",
    "    def __init__(self, sigma, theta):\n",
    "        if theta is None:\n",
    "            self.theta = 1 - 2*np.random.random()\n",
    "            self.sigma = np.random.random()\n",
    "        else:\n",
    "            self.theta = theta\n",
    "            self.sigma = sigma\n",
    "    \n",
    "    def draw_action(self,s):\n",
    "        a = np.random.normal(self.theta * s, self.sigma)\n",
    "        return a\n",
    "    \n",
    "    def gradient(self, a, s):\n",
    "        mu = self.theta * s\n",
    "        grad_mu = s\n",
    "        grad_theta = (a - mu)/(self.sigma**2) * s\n",
    "        grad_sigma = ((a 0 mu)**2 - self.sigma**2)/(sigma***3) * grad_mu\n",
    "        \n",
    "        return grad_theta, grad_sigma\n",
    "    \n",
    "    def update(self, delta_theta, delta_sigma):\n",
    "        self.theta += delta_theta\n",
    "        self.sigma += delta_sigma\n",
    "\n",
    "def REINFORCE():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Gradient-based update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard update rule\n",
    "\n",
    "class StandardStep(object):\n",
    "    def __init__(self, rate):\n",
    "        self.learning_rate = rate\n",
    "    \n",
    "    def update(self,alpha):\n",
    "        return alpha * self.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. Experiments: Linear-quadratic Gaussian regulation problem (LQG problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LQG definition\n",
    "\n",
    "R =  -0.5* (s.T @Q@ s + a.T @B@ a)\n",
    "A,B,Q,R = 1,1,1,1\n",
    "gamma = 0.9\n",
    "theta_s = -0.6\n",
    "T = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case considered: standard_deviation = 0.4\n",
    "\n",
    "sigma = 0.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
